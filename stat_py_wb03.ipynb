{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNGOT9LmeflzOyj8jCdbjeN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Masuzyo/Thrivesummer/blob/main/stat_py_wb03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lesson 3- Probability and Normal Distributions"
      ],
      "metadata": {
        "id": "pVYWPns02irj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_iris, load_diabetes, make_regression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import scipy.stats as stats\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "f_VumC4A2jvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Probability\n",
        "\n",
        "An Emprical distribution is a probability distribution we obtaian from looking at multiple observation of a in event.\n",
        "\n",
        "For example if we filp a coin 10 times"
      ],
      "metadata": {
        "id": "vSBSHyVS2_Q8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "sample_sizes=  [10,100,1000,100000]\n",
        "sample_space= ['Heads','Tails']\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for n in sample_sizes:\n",
        "    coin_flips=[random.choice(sample_space) for _ in range(n)]\n",
        "    axes[sample_sizes.index(n)].bar(['Heads','Tails'],[coin_flips.count('Heads'),coin_flips.count('Tails')])\n",
        "    axes[sample_sizes.index(n)].set_title(f'Sample Size = {n}')\n",
        "    axes[sample_sizes.index(n)].set_xlabel('Outcome')\n",
        "    axes[sample_sizes.index(n)].set_ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Lul0GofL46Bq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# roll a die 60 times and polt bargraph\n",
        "die_rolls = [random.randint(1, 6) for _ in range(60)]\n",
        "die_counts = pd.Series(die_rolls).value_counts()\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.barplot(x=die_counts.index, y=die_counts.values)\n",
        "plt.title('Die Roll Results (60 Rolls)')\n",
        "plt.xlabel('Outcome')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ITiFdQaR7dez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6000 times\n",
        "die_rolls = [random.randint(1, 6) for _ in range(6000)]\n",
        "die_counts = pd.Series(die_rolls).value_counts()\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.barplot(x=die_counts.index, y=die_counts.values)\n",
        "plt.title('Die Roll Results (6000 Rolls)')\n",
        "plt.xlabel('Outcome')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JM0mw6XeDPsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6000000 times\n",
        "die_rolls = [random.randint(1, 6) for _ in range(6000000)]\n",
        "die_counts = pd.Series(die_rolls).value_counts()\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.barplot(x=die_counts.index, y=die_counts.values)\n",
        "plt.title('Die Roll Results (6000000 Rolls)')\n",
        "plt.xlabel('Outcome')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "z-BtQHfQDcIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normal Distribution\n",
        "\n",
        "The normal distribution is a continuous probability distribution that is symmetric around its mean. It's characterized by two parameters: mean (μ) and standard deviation (σ).\n"
      ],
      "metadata": {
        "id": "SkjYoHBY3iF1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Extract sepal length (first feature)\n",
        "sepal_length = X[:, 0]\n",
        "\n",
        "# Plot histogram and normal distribution overlay\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(sepal_length, bins=20, density=True, alpha=0.7, color='skyblue')\n",
        "sns.kdeplot(sepal_length, color='blue', linewidth=2)\n",
        "plt.xlabel('Sepal Length')\n",
        "plt.ylabel('Density')\n",
        "plt.title('Distribution of Sepal Length')\n",
        "\n",
        "# Overlay normal distribution\n",
        "mu, sigma = np.mean(sepal_length), np.std(sepal_length)\n",
        "x = np.linspace(sepal_length.min(), sepal_length.max(), 100)\n",
        "plt.plot(x, stats.norm.pdf(x, mu, sigma), 'r-', label=f'Normal(μ={mu:.2f}, σ={sigma:.2f})')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KhzkB8u_27dd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Shapiro-wilk Test\n",
        "\n",
        "We use the shapiro-wilks test to see if based on the emp"
      ],
      "metadata": {
        "id": "TX4jzF3GFODU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#shapiro wilks\n",
        "res=stats.shapiro(sepal_length)\n",
        "print(\"p-value = \",res[1])\n"
      ],
      "metadata": {
        "id": "xKAEZPzrFuBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 1: Normal Distribution Analysis\n",
        "**Task**: Using the diabetes dataset, analyze the distribution of the target variable (disease progression).\n",
        "\n",
        "1. Load the diabetes dataset\n",
        "2. Extract the target variable\n",
        "3. Calculate mean and standard deviation\n",
        "4. Create a histogram with normal distribution overlay\n",
        "5. Perform a Shapiro-Wilk test to check if the data is normally distributed\n",
        "6. Interpret the results\n",
        "\n",
        "**Your Code Here:**"
      ],
      "metadata": {
        "id": "qasPBNIFEqIq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load diabetes dataset\n",
        "diabetes = load_diabetes()\n",
        "# Your solution here"
      ],
      "metadata": {
        "id": "wZWCi_SNE_if"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Central Limit Theorem\n",
        "\n",
        "### Theory\n",
        "The Central Limit Theorem states that the sampling distribution of the sample mean approaches a normal distribution as the sample size increases, regardless of the population's distribution."
      ],
      "metadata": {
        "id": "9qfB0qLqLjYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "diabetes = load_diabetes()\n",
        "population = diabetes.target\n",
        "\n",
        "# Demonstrate CLT with different sample sizes\n",
        "sample_sizes = [5, 10, 30, 1000]\n",
        "n_samples = 5000\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "axes = axes.ravel()\n",
        "\n",
        "fig.suptitle('Central Limit Theorem Demonstration', fontsize=16)\n",
        "\n",
        "for i, n in enumerate(sample_sizes):\n",
        "    sample_means = []\n",
        "    samples_all=[]\n",
        "    for _ in range(n_samples):\n",
        "        sample = np.random.choice(population, size=n, replace=True)\n",
        "        sample_means.append(np.mean(sample))\n",
        "        samples_all.append(sample)\n",
        "\n",
        "    res=stats.shapiro(sample_means)\n",
        "\n",
        "    axes[i].hist(sample_means, bins=30, density=True, alpha=0.7)\n",
        "    axes[i].set_title(f'Sample Size = {n},p-value={res[1]:.3f}')\n",
        "    axes[i].set_xlabel('Sample Mean')\n",
        "    axes[i].set_ylabel('Density')\n",
        "\n",
        "\n",
        "    # Overlay theoretical normal distribution\n",
        "    mu_theory = np.mean(population)\n",
        "    sigma_theory = np.std(population) / np.sqrt(n)\n",
        "    x = np.linspace(min(sample_means), max(sample_means), 100)\n",
        "    axes[i].plot(x, stats.norm.pdf(x, mu_theory, sigma_theory), 'r-', linewidth=2)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "E6ImORQMLuFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CLt example sampled from chisq distribution\n",
        "\n",
        "sample_sizes=[5,10,50,1000]\n",
        "n_samples=5000\n",
        "k=1\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for i, n in enumerate(sample_sizes):\n",
        "    sample_means = []\n",
        "    for _ in range(n_samples):\n",
        "        sample = np.random.chisquare(df=k, size=n)\n",
        "        sample_means.append(np.mean(sample))\n",
        "\n",
        "    res=stats.shapiro(sample_means)\n",
        "\n",
        "    axes[i].hist(sample_means, bins=30, density=True, alpha=0.7)\n",
        "    axes[i].set_title(f'Sample Size = {n},p-value={res[1]:.3f}')\n",
        "    axes[i].set_xlabel('Sample Mean')\n",
        "    axes[i].set_ylabel('Density')\n",
        "    axes[i].grid(True)\n",
        "\n",
        "    # Overlay theoretical normal distribution\n",
        "    mu_theory = k\n",
        "    sigma_theory = np.sqrt(2 * k) / np.sqrt(n)\n",
        "\n",
        "    x = np.linspace(min(sample_means), max(sample_means), 100)\n",
        "    axes[i].plot(x, stats.norm.pdf(x, mu_theory, sigma_theory), 'r-', linewidth=2)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "0ShEWgymRB8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 2: Central Limit Theorem Verification\n",
        "**Task**: Using the iris dataset, demonstrate the Central Limit Theorem using petal width measurements.\n",
        "\n",
        "1. Extract petal width data from the iris dataset\n",
        "2. Create sampling distributions for sample sizes: 3, 10, 25, 50\n",
        "3. Plot histograms showing how the sampling distribution becomes more normal\n",
        "4. Calculate the theoretical and empirical means and standard deviations\n",
        "5. Comment on how the sampling distribution changes with sample size\n",
        "\n",
        "**Your Code Here:**"
      ],
      "metadata": {
        "id": "cKmHI9yzOOwC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load iris dataset\n",
        "iris = load_iris()\n",
        "# Your solution here"
      ],
      "metadata": {
        "id": "VBu__CvuOoOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Confidence Intervals\n",
        "\n",
        "### Theory\n",
        "A confidence interval provides a range of values that likely contains the true population parameter with a specified level of confidence."
      ],
      "metadata": {
        "id": "21BjveJSO1tL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load iris dataset\n",
        "iris = load_iris()\n",
        "sepal_length = iris.data[:, 0]\n",
        "\n",
        "# Calculate 95% confidence interval for the mean\n",
        "n = len(sepal_length)\n",
        "mean = np.mean(sepal_length)\n",
        "std_err = stats.sem(sepal_length)  # Standard error of the mean\n",
        "\n",
        "# 95% confidence interval\n",
        "confidence_level = 0.95\n",
        "alpha = 1 - confidence_level\n",
        "t_critical = stats.t.ppf(1 - alpha/2, df=n-1)\n",
        "\n",
        "margin_of_error = t_critical * std_err\n",
        "ci_lower = mean - margin_of_error\n",
        "ci_upper = mean + margin_of_error\n",
        "\n",
        "print(f\"Sample mean: {mean:.3f}\")\n",
        "print(f\"95% Confidence Interval: ({ci_lower:.3f}, {ci_upper:.3f})\")\n",
        "print(f\"Margin of Error: {margin_of_error:.3f}\")"
      ],
      "metadata": {
        "id": "_nlGPbdDPK5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Confidence Interval for Proportion"
      ],
      "metadata": {
        "id": "pCMm4G46P3m0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a binary variable: large sepal length (> median)\n",
        "iris = load_iris()\n",
        "sepal_length = iris.data[:, 0]\n",
        "median_sepal = np.median(sepal_length)\n",
        "large_sepal = sepal_length > median_sepal\n",
        "\n",
        "# Calculate proportion and confidence interval using statsmodels\n",
        "from statsmodels.stats.proportion import proportion_confint\n",
        "import statsmodels.stats.proportion as smp\n",
        "\n",
        "n = len(large_sepal)\n",
        "x = np.sum(large_sepal)  # Number of successes\n",
        "p_hat = x / n\n",
        "\n",
        "print(f\"Sample proportion (large sepal): {p_hat:.3f}\")\n",
        "\n",
        "# 95% confidence interval for proportion using Wilson method (more accurate)\n",
        "ci_lower_prop, ci_upper_prop = proportion_confint(x, n, alpha=0.05, method='wilson')\n",
        "print(f\"95% Confidence Interval for Proportion: ({ci_lower_prop:.3f}, {ci_upper_prop:.3f})\")\n",
        "\n",
        "# Alternative: using normal approximation\n",
        "ci_lower_normal, ci_upper_normal = proportion_confint(x, n, alpha=0.05, method='normal')\n",
        "print(f\"95% CI (Normal approximation): ({ci_lower_normal:.3f}, {ci_upper_normal:.3f})\")"
      ],
      "metadata": {
        "id": "QkQyyA0bPzY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Confidence Interval for Difference in Means\n"
      ],
      "metadata": {
        "id": "SG9RF-hnQYqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare sepal length between two species\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "setosa_sepal = X[y == 0, 0]\n",
        "versicolor_sepal = X[y == 1, 0]\n",
        "\n",
        "# Using scipy.stats for confidence interval of difference in means\n",
        "# First, perform the t-test to get the confidence interval directly\n",
        "t_stat, p_value = stats.ttest_ind(setosa_sepal, versicolor_sepal)\n",
        "\n",
        "# Calculate confidence interval for difference using scipy\n",
        "from scipy.stats import ttest_ind_from_stats\n",
        "\n",
        "mean1, mean2 = np.mean(setosa_sepal), np.mean(versicolor_sepal)\n",
        "std1, std2 = np.std(setosa_sepal, ddof=1), np.std(versicolor_sepal, ddof=1)\n",
        "n1, n2 = len(setosa_sepal), len(versicolor_sepal)\n",
        "\n",
        "# Using built-in function for confidence interval\n",
        "result = stats.ttest_ind(setosa_sepal, versicolor_sepal)\n",
        "ci_lower_diff, ci_upper_diff = result.confidence_interval(confidence_level=0.95)\n",
        "\n",
        "print(f\"Setosa mean: {mean1:.3f}, Versicolor mean: {mean2:.3f}\")\n",
        "print(f\"Difference in means: {mean1 - mean2:.3f}\")\n",
        "print(f\"95% CI for difference: ({ci_lower_diff:.3f}, {ci_upper_diff:.3f})\")\n",
        "print(f\"t-statistic: {t_stat:.3f}, p-value: {p_value:.6f}\")"
      ],
      "metadata": {
        "id": "lnTaFCACQXJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 3: Confidence Interval Construction\n",
        "**Task**: Using the diabetes dataset, construct different types of confidence intervals.\n",
        "\n",
        "1. **Mean CI**: Calculate 90%, 95%, and 99% confidence intervals for the mean target variable\n",
        "2. **Proportion CI**: Create a binary variable (e.g., high disease progression > 140) and calculate confidence intervals for the proportion using Wilson and normal methods\n",
        "3. **Difference CI**: Split the data by a feature (e.g., gender) and calculate confidence intervals for the difference in means\n",
        "4. **Variance CI**: Calculate a confidence interval for the population variance using chi-square distribution\n",
        "5. Create visualizations showing how confidence intervals change with different confidence levels and sample sizes\n",
        "\n",
        "**Your Code Here:**"
      ],
      "metadata": {
        "id": "oAaQYuUAU7Ei"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OS1HMoEGVJKh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}